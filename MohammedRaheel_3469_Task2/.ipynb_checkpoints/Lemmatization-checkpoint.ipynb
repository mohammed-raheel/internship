{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'go', 'go', 'go', 'go']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"going\",\"gone\",\"go\",\"goes\",\"went\"]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Here pos='v' indicates verb\n",
    "# pos stands for Part-of-speech\n",
    "tokens_new = [lemmatizer.lemmatize(token,pos='v') for token in tokens]\n",
    "tokens_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man', 'woman', 'man', 'woman', 'bat', 'bat', 'leg', 'leg']\n"
     ]
    }
   ],
   "source": [
    "# Nouns. By default, lemmatizer looks for nouns. As shown above, we have explicitly called Verb\n",
    "tokens = ['man','woman','mans','womans','bat','bats','leg','legs']\n",
    "x = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNOWBALL STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use algorithm to get to base form by removing prefixes and suffixes\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man', 'woman', 'man', 'woman', 'bat', 'bat', 'leg', 'leg', 'hand', 'fli', 'run', 'get']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['man','woman','mans','womans','bat','bats','leg','legs','hands','flying','running','getting']\n",
    "snow = SnowballStemmer('english')\n",
    "x = [snow.stem(token) for token in tokens]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PORTER STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an older version of stemmer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man', 'woman', 'man', 'woman', 'bat', 'bat', 'leg', 'leg', 'hand', 'fli', 'run', 'get']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "y = [porter.stem(token) for token in tokens]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N GRAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you classify or group your documents/ tweets based on these? Do single words make sense? Need to capture the context between words. Hence, we have to use ngrams to capture two or three or four words together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('man', 'woman')\n",
      "('woman', 'man')\n",
      "('man', 'woman')\n",
      "('woman', 'bat')\n",
      "('bat', 'bat')\n",
      "('bat', 'leg')\n",
      "('leg', 'leg')\n",
      "('leg', 'hand')\n",
      "('hand', 'fli')\n",
      "('fli', 'run')\n",
      "('run', 'get')\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Bi Gram\n",
    "count = 0\n",
    "for i in nltk.ngrams(y,2):\n",
    "    print(i)\n",
    "    count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('man', 'woman', 'man')\n",
      "('woman', 'man', 'woman')\n",
      "('man', 'woman', 'bat')\n",
      "('woman', 'bat', 'bat')\n",
      "('bat', 'bat', 'leg')\n",
      "('bat', 'leg', 'leg')\n",
      "('leg', 'leg', 'hand')\n",
      "('leg', 'hand', 'fli')\n",
      "('hand', 'fli', 'run')\n",
      "('fli', 'run', 'get')\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Tri Gram\n",
    "count = 0\n",
    "for i in nltk.ngrams(y,3):\n",
    "    print(i)\n",
    "    count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find frequency of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.probability.FreqDist'>\n",
      "[(('man', 'woman'), 2), (('woman', 'man'), 1), (('woman', 'bat'), 1), (('bat', 'bat'), 1), (('bat', 'leg'), 1), (('leg', 'leg'), 1), (('leg', 'hand'), 1), (('hand', 'fli'), 1), (('fli', 'run'), 1), (('run', 'get'), 1)]\n"
     ]
    }
   ],
   "source": [
    "frequency = nltk.FreqDist()\n",
    "for i in nltk.ngrams(y,2):\n",
    "    frequency[i]+=1\n",
    "\n",
    "print(frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
    "My companion Mr. Alfred sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
    "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
    "At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. My flight is at 09:30 AM and I have to reach airport by 08:00 AM'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import RegexpTokenizer as regExToken\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "lmtzr = WordNetLemmatizer()\n",
    "reg = regExToken('\\w+')\n",
    "def process_text(text):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    #sentence_tokens = [tokenizer.tokenize(sentence) for sentence in sentences] # list of lists\n",
    "    tokens = [] # initialising list variable\n",
    "    \n",
    "    #for sentence in sentence_tokens:\n",
    "    #    sent = []\n",
    "    #    for word in sentence:\n",
    "    #        if word.lower() not in stop:\n",
    "    #            sent.append(word.lower())\n",
    "    #    tokens.append(sent)\n",
    "    \n",
    "    for i in sentences:\n",
    "        sent = []\n",
    "        words = reg.tokenize(i)\n",
    "        for i in words:\n",
    "            if i.lower() not in stop:\n",
    "                sent.append(i.lower())\n",
    "        tokens.append(sent)\n",
    "    \n",
    "    ## THE SAME for LOOP CAN BE WRITTEN AS FOLLOWS\n",
    "    ## tokens = [[word.lower() for word in sent if word not in stop] for sent in sentence_tokens]\n",
    "    \n",
    "    tokens = [[lmtzr.lemmatize(word) for word in sent] for sent in tokens]\n",
    "    return tokens\n",
    "\n",
    "def process_ngrams(input_sentence_tokens):\n",
    "    ngram_list = []\n",
    "    for sentence in input_sentence_tokens:\n",
    "        ngram_sent = nltk.ngrams(sentence, 2)\n",
    "        ngram_list = ngram_list + list(ngram_sent)\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'lovely', 'surrey', 'lane'], ['perfect', 'day', 'bright', 'sun', 'fleecy', 'cloud', 'heaven'], ['tree', 'wayside', 'hedge', 'throwing', 'first', 'green', 'shoot', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth'], ['least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged'], ['companion', 'mr', 'alfred', 'sat', 'front', 'trap', 'arm', 'folded', 'hat', 'pulled', 'eye', 'chin', 'sunk', 'upon', 'breast', 'buried', 'deepest', 'thought'], ['suddenly', 'however', 'started', 'tapped', 'shoulder', 'pointed', 'meadow'], ['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'lovely', 'surrey', 'lane'], ['perfect', 'day', 'bright', 'sun', 'fleecy', 'cloud', 'heaven'], ['flight', '09', '30', 'reach', 'airport', '08', '00']]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = process_text(txt)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
